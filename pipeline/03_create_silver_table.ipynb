{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19821fd-43ad-402c-b856-2a820d70918f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./00_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1ae5195-2105-4da5-b6e3-d99b400b132a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# silverテーブルを作成\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {silver_table_path}\n",
    "    (\n",
    "        `Timestamp` TIMESTAMP,\n",
    "        `Temperature` FLOAT,\n",
    "        `Humidity` FLOAT,\n",
    "        _datasource STRING,\n",
    "        _ingest_timestamp timestamp\n",
    "    )\n",
    "    USING DELTA\n",
    "    \"\"\"\n",
    ")\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d801c577-166a-4e4c-814e-be27e54ba4e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. `bronze`テーブルから主キー（`Timestamp`）ごとに`_ingest_timestamp`列の最大日を抽出したサブセットを作成\n",
    "2. 主キー＋`_ingest_timestamp`列の条件で、1のサブセットと`bronze`テーブルを結合\n",
    "3. `bronze`テーブルのデータ型をシルバーテーブルと同一のデータ型に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0ad08ff-1fd1-4fe4-b5e6-24a5159f46da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "bronze_to_silver_sql = f'''\n",
    "with latest_bronze_records (\n",
    "    SELECT\n",
    "        Timestamp,\n",
    "        MAX(_ingest_timestamp) AS max_ingest_timestamp\n",
    "        FROM\n",
    "            {bronze_table_path}\n",
    "        GROUP BY\n",
    "            Timestamp\n",
    ")\n",
    "SELECT\n",
    "    bronze.`Timestamp`::timestamp,\n",
    "    bronze.`Temperature`::FLOAT,\n",
    "    bronze.`Humidity`::FLOAT,\n",
    "    bronze._datasource,\n",
    "    bronze._ingest_timestamp::timestamp\n",
    "    \n",
    "    FROM\n",
    "        {bronze_table_path} AS bronze\n",
    "    INNER JOIN \n",
    "        latest_bronze_records AS latest_bronze\n",
    "        ON \n",
    "            bronze.Timestamp =  latest_bronze.Timestamp\n",
    "            AND bronze._ingest_timestamp =  latest_bronze.max_ingest_timestamp\n",
    "'''\n",
    "df = spark.sql(bronze_to_silver_sql)\n",
    "\n",
    "# dropDuplicates関数にて、主キーの一意性を保証。連携日ごとの一意性が保証されないことがあるため。\n",
    "df = df.drop_duplicates(['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e8efce4-b6d6-46c9-92b7-1e46857f31f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 処理後の結果を確認\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcb76671-4b9c-4ccd-8fe5-218d11db15ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 一時ビューから`product2__silver`に対して、MERGE文によりアップサート処理を実施。\n",
    "## 一時ビューを作成\n",
    "df_view_name = f'_tmp_silver'\n",
    "df.createOrReplaceTempView(df_view_name)\n",
    "\n",
    "## Merge処理を実行\n",
    "returned_df = spark.sql(f'''\n",
    "MERGE INTO {silver_table_path} AS silver\n",
    "  USING {df_view_name} AS src\n",
    "  \n",
    "  ON silver.Timestamp = src.Timestamp\n",
    "\n",
    "  WHEN MATCHED\n",
    "  AND silver._ingest_timestamp < src._ingest_timestamp\n",
    "    THEN UPDATE SET *\n",
    "  WHEN NOT MATCHED\n",
    "    THEN INSERT *\n",
    "''')\n",
    "returned_df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_create_silver_table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
